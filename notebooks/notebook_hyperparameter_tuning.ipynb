{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## GRPO - hyperparameter-tuning (sp-struct-rwd1)"
      ],
      "metadata": {
        "id": "y6uqM1dRq5wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1"
      ],
      "metadata": {
        "id": "wCl8NYKcq_3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ],
      "metadata": {
        "id": "PTU76G7KrD2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import wandb and swi-prolog"
      ],
      "metadata": {
        "id": "d1M1BuAGWKGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "!apt-get install swi-prolog #for colab\n",
        "#!conda install -y -c conda-forge swi-prolog #for ucloud"
      ],
      "metadata": {
        "id": "fCpOc51w5Ukr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System prompt"
      ],
      "metadata": {
        "id": "TjurghNsWM_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a specialized Prolog code-generating assistant.\n",
        "\n",
        "Your task is to solve math problems by providing a structured answer in two clearly defined sections:\n",
        "\n",
        "1. <reasoning>\n",
        "   - Provide a clear, concise step-by-step explanation of how you arrive at the solution.\n",
        "\n",
        "2. <answer>\n",
        "   - Provide executable Prolog code using constraint logic programming to compute the numeric answer.\n",
        "   - Always start with: ':- use_module(library(clpq)).'\n",
        "   - Define any necessary numeric constants or intermediate values using predicates.\n",
        "   - Final answer should be unified explicitly in solve(X) using curly-brace constraints, without printing commands.\n",
        "\n",
        "Use this XML format strictly:\n",
        "<reasoning>\n",
        "(Your step-by-step reasoning here)\n",
        "</reasoning>\n",
        "<answer>\n",
        ":- use_module(library(clpq)).\n",
        "\n",
        "(Any predicates/constants defined here)\n",
        "\n",
        "solve(X) :-\n",
        "    (Intermediate computations using curly braces)\n",
        "    {X = final constraint logic}.\n",
        "</answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "r0xbS71x5Z0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWauTSEeDkC9"
      },
      "source": [
        "### Preprocess dataset and push to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E55C30q3g9VG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import load_dataset\n",
        "import subprocess\n",
        "\n",
        "def execute_prolog_code(prolog_code: str) -> str:\n",
        "    \"\"\"\n",
        "    Executes the given Prolog code in SWI-Prolog, calling solve(X),\n",
        "    and returns the printed solution as a string (e.g., \"12000\").\n",
        "    Returns None if there's an error or no output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Write the Prolog code to a temporary file\n",
        "        with open(\"temp.pl\", \"w\") as f:\n",
        "            f.write(prolog_code)\n",
        "\n",
        "        # Run SWI-Prolog: load 'temp.pl', call solve(X), print X, then halt\n",
        "        result = subprocess.run(\n",
        "            [\"swipl\", \"-q\", \"-f\", \"temp.pl\", \"-g\", \"solve(X), writeln(X), halt\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=5,  # optional: 5-second timeout\n",
        "        )\n",
        "\n",
        "        # If there's any error output, we can check result.stderr or result.returncode\n",
        "        if result.returncode != 0 or not result.stdout:\n",
        "            return None\n",
        "\n",
        "        # result.stdout is whatever got printed by writeln(X)\n",
        "        lines = result.stdout.strip().splitlines()\n",
        "        return lines[-1].strip() if lines else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing Prolog code: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCizrIcZg_3X"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def get_gsm8k_questions(split=\"train\"):\n",
        "    data = load_dataset('niklasm222/gsm8k-prolog-prover')[split]\n",
        "\n",
        "    def map_fn(x):\n",
        "        # Compute the correct numerical result by executing the reference Prolog solution.\n",
        "        numerical_result = execute_prolog_code(x[\"output\"])\n",
        "        return {\n",
        "            \"instruction\": x[\"instruction\"],\n",
        "            \"input\": x[\"input\"],\n",
        "            \"output\": x[\"output\"],\n",
        "            \"prompt\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"{x['instruction']}\\n{x['input']}\"}\n",
        "            ],\n",
        "            # Optionally, you can also append the numerical result to the output field.\n",
        "            \"answer\": x['output'],\n",
        "            \"numerical_result\": str(numerical_result),  # Precomputed numeric result\n",
        "        }\n",
        "\n",
        "    data = data.map(map_fn)\n",
        "    return data\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxyyBwfDhCBV"
      },
      "outputs": [],
      "source": [
        "# Save and push the dataset to Hugging Face Hub.\n",
        "# Replace \"your_username\" with your HF username and \"hf_your_token\" with your token if needed.\n",
        "dataset.push_to_hub(\"niklasm222/gsm8k-prolog-prover-sp_struct-v4\", token=\"\", private=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "vbqv485yWO2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import re\n",
        "import subprocess\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import TrainerCallback, TrainerState, TrainerControl, TrainingArguments\n",
        "from transformers.integrations import WandbCallback\n",
        "\n",
        "from transformers import EvalPrediction\n",
        "import inspect\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "def get_gsm8k_split(subset_size=2500, seed=42):\n",
        "    \"\"\"\n",
        "    Load the 'niklasm222/gsm8k-prolog-prover-v4' dataset, select a subset,\n",
        "    and split it into 70% train, 15% validation, and 15% test.\n",
        "    \"\"\"\n",
        "    # 1. Load dataset and shuffle\n",
        "    dataset = load_dataset(\"niklasm222/gsm8k-prolog-prover-sp_struct-v4\", split=\"train\")\n",
        "    subset = dataset.shuffle(seed=seed).select(range(subset_size))\n",
        "\n",
        "    # 2. Split off 15% for test\n",
        "    split_1 = subset.train_test_split(test_size=0.15, seed=seed)\n",
        "    train_val = split_1[\"train\"]\n",
        "    test = split_1[\"test\"]\n",
        "\n",
        "    # 3. From the remaining 85%, split off 15% for validation (~0.1765)\n",
        "    val_ratio = 0.15 / 0.85\n",
        "    split_2 = train_val.train_test_split(test_size=val_ratio, seed=seed)\n",
        "    train = split_2[\"train\"]\n",
        "    val = split_2[\"test\"]\n",
        "\n",
        "    return DatasetDict({\"train\": train, \"validation\": val, \"test\": test})\n",
        "\n",
        "# Load data\n",
        "splits = get_gsm8k_split()\n",
        "train_dataset = splits[\"train\"]\n",
        "val_dataset = splits[\"validation\"]\n",
        "test_dataset = splits[\"test\"]\n",
        "\n",
        "# ---------------------\n",
        "# Reward Functions\n",
        "# ---------------------\n",
        "\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    \"\"\"\n",
        "    1) Truncate 'text' at <|endoftext|> if present.\n",
        "    2) Find the FIRST fully-completed <answer>...</answer> block in that truncated text.\n",
        "    3) Return that block's content, or None if not found.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1) Truncate at <|endoftext|>\n",
        "        eot_index = text.find(\"<|endoftext|>\")\n",
        "        truncated_text = text[:eot_index] if eot_index != -1 else text\n",
        "\n",
        "        # 2) Find the FIRST <answer> tag\n",
        "        start = truncated_text.find(\"<answer>\")\n",
        "        if start == -1:\n",
        "            return None\n",
        "\n",
        "        # 3) Find the NEXT </answer> after this <answer>\n",
        "        end = truncated_text.find(\"</answer>\", start)\n",
        "        if end == -1:\n",
        "            return None\n",
        "\n",
        "        return truncated_text[start+len(\"<answer>\"):end].strip()\n",
        "\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def execute_prolog_code(prolog_code: str) -> str:\n",
        "    try:\n",
        "        with open(\"temp.pl\", \"w\") as f:\n",
        "            f.write(prolog_code)\n",
        "        result = subprocess.run(\n",
        "            [\"swipl\", \"-q\", \"-f\", \"temp.pl\", \"-g\", \"solve(X), writeln(X), halt\"],\n",
        "            capture_output=True, text=True, timeout=5,\n",
        "        )\n",
        "        if result.returncode != 0 or not result.stdout:\n",
        "            return None\n",
        "        lines = result.stdout.strip().splitlines()\n",
        "        return lines[-1].strip() if lines else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing Prolog code: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ---------------------\n",
        "# Reward Functions\n",
        "# ---------------------\n",
        "def correctness_reward_func(prompts, completions, answer, numerical_result, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Compare the model’s executed Prolog answer to the known correct numeric result.\n",
        "    Provide partial rewards for progress toward correctness during early training.\n",
        "    This version does NOT rely on 'validate_prolog_code',\n",
        "    and instead depends on SWI-Prolog execution results.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Get the model's generated text and extract the Prolog snippet\n",
        "    responses = [comp[0][\"content\"] for comp in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "\n",
        "    # 2. Retrieve reference numeric results (passed from dataset)\n",
        "    correct_values = numerical_result\n",
        "\n",
        "    # 3. Debug print for the first sample only\n",
        "    if len(responses) > 0:\n",
        "        question = prompts[0][-1][\"content\"] if (prompts and prompts[0]) else \"N/A\"\n",
        "        print(\n",
        "            \"-\" * 20,\n",
        "            f\"Question:\\n{question}\",\n",
        "            f\"\\nReference Prolog answer:\\n{answer[0]}\",\n",
        "            f\"\\nReference Numerical Result:\\n{correct_values[0]}\",\n",
        "            f\"\\nModel Response:\\n{responses[0]}\",\n",
        "            f\"\\nExtracted Code:\\n{extracted_responses[0]}\"\n",
        "        )\n",
        "\n",
        "    # 4. Execute the model's Prolog code with SWI-Prolog\n",
        "    model_values = []\n",
        "    for code in extracted_responses:\n",
        "        if code:\n",
        "            mv = execute_prolog_code(code)\n",
        "            if mv:\n",
        "                model_values.append(mv)\n",
        "            else:\n",
        "                model_values.append(None)\n",
        "                print(\"SWI-Prolog returned no output or an error.\")\n",
        "        else:\n",
        "            model_values.append(None)\n",
        "            print(\"No Prolog code extracted from the model.\")\n",
        "\n",
        "    # 5. Compare results and provide rewards\n",
        "    rewards = []\n",
        "    for mv, cv in zip(model_values, correct_values):\n",
        "        if mv is None or cv is None:\n",
        "            # Partial reward for at least attempting to generate some code\n",
        "            rewards.append(0.5)\n",
        "            print(\"Partial Reward: Model attempted code or code is None, no numeric match.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # If it's an unbound variable, e.g. \"_12345\", that's partial credit\n",
        "            if mv.startswith(\"_\"):\n",
        "                rewards.append(0.5)\n",
        "                print(f\"Unbound variable in Prolog output: {mv}\")\n",
        "                continue\n",
        "\n",
        "            mv_cleaned = mv.strip().split('\\n')[-1]\n",
        "            mv_float = float(mv_cleaned)\n",
        "            cv_float = float(cv)\n",
        "            print(f\"Model Value: {mv_float}, Correct Value: {cv_float}\")\n",
        "\n",
        "            if abs(mv_float - cv_float) < 1e-6:\n",
        "                # Full reward for correct numeric result\n",
        "                rewards.append(2.0)\n",
        "                print(\"Match: Model value matches correct value.\")\n",
        "            else:\n",
        "                # Partial reward for producing a numeric result, but not correct\n",
        "                rewards.append(1.0)\n",
        "                print(\"Partial Reward: Model generated a numeric result, but it's incorrect.\")\n",
        "        except Exception as e:\n",
        "            # Partial credit for at least generating code that runs\n",
        "            rewards.append(0.5)\n",
        "            print(f\"Error converting model output to float: {e}\\nModel: {mv}, Correct: {cv}\")\n",
        "\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def prolog_syntax_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Partial reward for including Prolog-specific patterns:\n",
        "      - ':-' (typical directives, e.g. :- use_module)\n",
        "      - 'solve('\n",
        "      - lines ending with '.'\n",
        "      - 'use_module(library(clpq))'\n",
        "    \"\"\"\n",
        "    pattern = r'(?::-|solve\\s*\\(|use_module|clpq|\\.\\s*$)'\n",
        "    rewards = []\n",
        "    for c in completions:\n",
        "        text = c[0][\"content\"]\n",
        "        hits = re.findall(pattern, text, re.MULTILINE)\n",
        "        # Simple approach: #hits * 0.2, capped at 1.0\n",
        "        score = min(len(hits) * 0.2, 1.0)\n",
        "        rewards.append(score)\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    \"\"\"Reward function that checks if the completion has a CoT-like XML format.\"\"\"\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "    matches = [re.search(pattern, r, flags=re.DOTALL) for r in responses]\n",
        "    return [0.5 if match else 0.0 for match in matches]\n",
        "\n",
        "def count_xml(text: str) -> float:\n",
        "    \"\"\"\n",
        "    A custom function that attempts to parse how well the output\n",
        "    adheres to your <reasoning>...</reasoning> <answer>...</answer> blocks.\n",
        "    \"\"\"\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1) * 0.001\n",
        "    return count\n",
        "\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [completion[0][\"content\"] for completion in completions]\n",
        "    return [count_xml(c) for c in contents]\n",
        "\n",
        "# -----------------------------------------\n",
        "# Load model and tokenizer\n",
        "# -----------------------------------------\n",
        "def train():\n",
        "    with wandb.init() as run:\n",
        "        config = wandb.config\n",
        "\n",
        "        # Load Data as subset\n",
        "        splits = get_gsm8k_split(subset_size=100, seed=42)\n",
        "        train_dataset = splits[\"train\"]\n",
        "        val_dataset = splits[\"validation\"]\n",
        "        test_dataset = splits[\"test\"]\n",
        "\n",
        "        # Build the base Qwen model & tokenizer\n",
        "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "            model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
        "            max_seq_length=2048,\n",
        "            load_in_4bit=True,\n",
        "            fast_inference=True,\n",
        "            max_lora_rank=64,\n",
        "            gpu_memory_utilization=0.7,\n",
        "        )\n",
        "\n",
        "        # Apply LoRA with hyperparams from config\n",
        "        model = FastLanguageModel.get_peft_model(\n",
        "            model,\n",
        "            r=config.r,\n",
        "            target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "            lora_alpha=config.lora_alpha,\n",
        "            use_gradient_checkpointing=\"unsloth\",\n",
        "            random_state=3407,\n",
        "        )\n",
        "\n",
        "        # Set GRPO training args\n",
        "        training_args = GRPOConfig(\n",
        "            use_vllm=True,\n",
        "            learning_rate=config.learning_rate,\n",
        "            adam_beta1=0.9,\n",
        "            adam_beta2=0.99,\n",
        "            weight_decay=0.1,\n",
        "            warmup_ratio=0.1,\n",
        "            lr_scheduler_type=\"cosine\",\n",
        "            optim=\"adamw_8bit\",\n",
        "            logging_steps=1,\n",
        "            bf16=is_bfloat16_supported(),\n",
        "            fp16=not is_bfloat16_supported(),\n",
        "            per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "            per_device_eval_batch_size=config.per_device_train_batch_size,\n",
        "            gradient_accumulation_steps=1,\n",
        "            num_generations=config.num_generations,\n",
        "            max_prompt_length=256,\n",
        "            max_completion_length=768,\n",
        "            num_train_epochs=1,\n",
        "            #max_steps=1000,\n",
        "            save_steps=250,\n",
        "            max_grad_norm=config.max_grad_norm,\n",
        "            report_to=\"wandb\",\n",
        "            output_dir=\"outputs_temp\",\n",
        "            eval_strategy=\"epoch\",\n",
        "\n",
        "        )\n",
        "\n",
        "        eval_bs = training_args.per_device_eval_batch_size\n",
        "\n",
        "        # Create the RL Trainer\n",
        "        trainer = GRPOTrainer(\n",
        "            model=model,\n",
        "            processing_class=tokenizer,\n",
        "            reward_funcs=[\n",
        "                xmlcount_reward_func,\n",
        "                soft_format_reward_func,\n",
        "                strict_format_reward_func,\n",
        "                prolog_syntax_reward_func,\n",
        "                correctness_reward_func,\n",
        "            ],\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=val_dataset,\n",
        "        )\n",
        "\n",
        "        # 2) run\n",
        "        trainer.train()\n",
        "\n",
        "        final_metrics = trainer.evaluate()\n",
        "        print(f\"Final Evaluation: {final_metrics}\")\n",
        "        wandb.log(final_metrics)\n",
        "\n",
        "        # Generate a unique repository name for this run\n",
        "        run_name = wandb.run.name or wandb.run.id\n",
        "        hf_repo_name = f\"niklasm222/qwen2.5-3b-1.75k-prolog-sp-struct-rwd1-{run_name}\"\n",
        "\n",
        "        # Directly push the merged model (including LoRA) to Hugging Face Hub\n",
        "        model.push_to_hub_merged(\n",
        "            hf_repo_name,\n",
        "            tokenizer,\n",
        "            save_method=\"merged_16bit\",\n",
        "            token=\"\"\n",
        "        )\n",
        "        print(f\"Pushed merged model to Hugging Face Hub: {hf_repo_name}\")"
      ],
      "metadata": {
        "id": "7thUDHh7r7sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sweep configs"
      ],
      "metadata": {
        "id": "11RVmMTtyrus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\"name\": \"eval/rewards/correctness_reward_func\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        # Most sensitive—search log-uniformly\n",
        "        \"learning_rate\": {\n",
        "            \"min\": 5e-6,\n",
        "            \"max\": 1e-4,\n",
        "            \"distribution\": \"log_uniform_values\"\n",
        "        },\n",
        "        # LoRA capacity (rank r)\n",
        "        \"r\": {\n",
        "            \"values\": [32,64]\n",
        "        },\n",
        "        # LoRA scaling alpha\n",
        "        \"lora_alpha\": {\n",
        "            \"values\": [64]\n",
        "        },\n",
        "        # Batch vs. grad-accum trade-off\n",
        "        \"per_device_train_batch_size\": {\n",
        "            \"values\": [8, 16]\n",
        "        },\n",
        "        # How many rollouts per prompt\n",
        "        \"num_generations\": {\n",
        "            \"values\": [4, 8]\n",
        "        },\n",
        "        # Clip norm (caps runaway gradients)\n",
        "        \"max_grad_norm\": {\n",
        "            \"min\": 0.1,\n",
        "            \"max\": 1.0,\n",
        "            \"distribution\": \"uniform\"\n",
        "        },\n",
        "        # Weight decay regularization\n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0.01, 0.1, 0.2]\n",
        "        },\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "r-SWCn492DGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Launch sweep"
      ],
      "metadata": {
        "id": "ZxFycmG3yxFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------\n",
        "# Launch the sweep\n",
        "# -----------------------------------------\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"prolog-hyperparameter-bayes\")\n",
        "\n",
        "# run\n",
        "wandb.agent(sweep_id, function=train, count=12)"
      ],
      "metadata": {
        "id": "B47vUmqp2X_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
