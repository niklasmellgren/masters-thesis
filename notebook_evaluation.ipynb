{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook shows the evaluation for sp-struct-rwd1-multipletry.\n",
        "\n",
        "The evaluation design, system prompt and model can be replaced by our other designs and models:\n",
        "- sp-base, sp-declare, sp-reflect\n",
        "- singletry, agentic-internal, agentic-independent."
      ],
      "metadata": {
        "id": "zfBBDUU5B5BY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Unsloth and VLLM"
      ],
      "metadata": {
        "id": "H2Mojbzy_9MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1"
      ],
      "metadata": {
        "id": "UefSggFE_9MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install transformers==4.51.3\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ],
      "metadata": {
        "id": "dISQD8cT_9MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import wandb"
      ],
      "metadata": {
        "id": "WGJX6VKx_9ME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjXjP3vB_9ME"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install swi-prolog"
      ],
      "metadata": {
        "id": "RApDHAu-_9ME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### System prompt"
      ],
      "metadata": {
        "id": "WgdKX8JN_9ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a specialized Prolog code-generating assistant.\n",
        "\n",
        "Your task is to solve math problems by providing a structured answer in two clearly defined sections:\n",
        "\n",
        "1. <reasoning>\n",
        "   - Provide a clear, concise step-by-step explanation of how you arrive at the solution.\n",
        "\n",
        "2. <answer>\n",
        "   - Provide executable Prolog code using constraint logic programming to compute the numeric answer.\n",
        "   - Always start with: ':- use_module(library(clpq)).'\n",
        "   - Define any necessary numeric constants or intermediate values using predicates.\n",
        "   - Final answer should be unified explicitly in solve(X) using curly-brace constraints, without printing commands.\n",
        "\n",
        "Use this XML format strictly:\n",
        "<reasoning>\n",
        "(Your step-by-step reasoning here)\n",
        "</reasoning>\n",
        "<answer>\n",
        ":- use_module(library(clpq)).\n",
        "\n",
        "(Any predicates/constants defined here)\n",
        "\n",
        "solve(X) :-\n",
        "    (Intermediate computations using curly braces)\n",
        "    {X = final constraint logic}.\n",
        "</answer>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CXHqQLJ8_9MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess data"
      ],
      "metadata": {
        "id": "NRHCobcw_9MF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aepFk1uQ_9MF"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from datasets import load_dataset\n",
        "import subprocess\n",
        "\n",
        "\n",
        "def execute_prolog_code(prolog_code: str) -> str:\n",
        "    \"\"\"\n",
        "    Executes the given Prolog code in SWI-Prolog, calling solve(X),\n",
        "    and returns the printed solution as a string (e.g., \"12000\").\n",
        "    Returns None if there's an error or no output.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Write the Prolog code to a temporary file\n",
        "        with open(\"temp.pl\", \"w\") as f:\n",
        "            f.write(prolog_code)\n",
        "\n",
        "        # Run SWI-Prolog: load 'temp.pl', call solve(X), print X, then halt\n",
        "        result = subprocess.run(\n",
        "            [\"swipl\", \"-q\", \"-f\", \"temp.pl\", \"-g\", \"solve(X), writeln(X), halt\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            timeout=5,  # optional: 5-second timeout\n",
        "        )\n",
        "\n",
        "        # If there's any error output, we can check result.stderr or result.returncode\n",
        "        if result.returncode != 0 or not result.stdout:\n",
        "            return None\n",
        "\n",
        "        # result.stdout is whatever got printed by writeln(X)\n",
        "        lines = result.stdout.strip().splitlines()\n",
        "        return lines[-1].strip() if lines else None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing Prolog code: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vQcCmcY_9MF"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def get_gsm8k_questions(split=\"train\"):\n",
        "    data = load_dataset('niklasm222/gsm8k-prolog-prover')[split]\n",
        "\n",
        "    def map_fn(x):\n",
        "        # Compute the correct numerical result by executing the reference Prolog solution.\n",
        "        numerical_result = execute_prolog_code(x[\"output\"])\n",
        "        return {\n",
        "            \"instruction\": x[\"instruction\"],\n",
        "            \"input\": x[\"input\"],\n",
        "            \"output\": x[\"output\"],\n",
        "            \"prompt\": [\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": f\"{x['instruction']}\\n{x['input']}\"}\n",
        "            ],\n",
        "            # Optionally, you can also append the numerical result to the output field.\n",
        "            \"answer\": x['output'],\n",
        "            \"numerical_result\": str(numerical_result),  # Precomputed numeric result\n",
        "        }\n",
        "\n",
        "    data = data.map(map_fn)\n",
        "    return data\n",
        "\n",
        "dataset = get_gsm8k_questions()\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1AYjOXF_9MF"
      },
      "outputs": [],
      "source": [
        "# Save and push the dataset to Hugging Face Hub.\n",
        "# Replace \"your_username\" with your HF username and \"hf_your_token\" with your token if needed.\n",
        "dataset.push_to_hub(\"niklasm222/gsm8k-prolog-prover-sp_struct-v4\", token=\"\", private=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "M_zftGGA_9MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "\n",
        "# Optionally: pip install unsloth==2025.3.6 unsloth_zoo==2025.3.4 vllm\n",
        "# Then load your already merged 16-bit model\n",
        "model_name = \"niklasm222/qwen2.5-3b-inst-grpo-1.75k-gsm8k-sp_struct-rwd1-v4.2\"\n",
        "max_seq_length = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False,   # If you merged into 16-bit, just load in normal float16 or CPU\n",
        "    fast_inference = True,  # If you want to use vLLM for fast generation\n",
        "    gpu_memory_utilization = 0.7,\n",
        ")\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "icXkyCUI_9MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Qwen chat template"
      ],
      "metadata": {
        "id": "UVK-0tq0_9MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this to your code to inspect the chat template\n",
        "print(\"\\nQwen Chat Template:\")\n",
        "print(tokenizer.chat_template)"
      ],
      "metadata": {
        "id": "PHpZ5L3b_9MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "jYU_Ogx7_9MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import subprocess\n",
        "\n",
        "def get_gsm8k_split(subset_size=2500, seed=42):\n",
        "    \"\"\"\n",
        "    Load the dataset, select a subset,\n",
        "    and split it into 70% train, 15% validation, and 15% test.\n",
        "    \"\"\"\n",
        "    # 1. Load dataset and shuffle\n",
        "    dataset = load_dataset(\"niklasm222/gsm8k-prolog-prover-sp_struct-v4\", split=\"train\")\n",
        "    subset = dataset.shuffle(seed=seed).select(range(subset_size))\n",
        "\n",
        "    # 2. Split off 15% for test\n",
        "    split_1 = subset.train_test_split(test_size=0.15, seed=seed)\n",
        "    train_val = split_1[\"train\"]\n",
        "    test = split_1[\"test\"]\n",
        "\n",
        "    # 3. From the remaining 85%, split off 15% for validation (~0.1765)\n",
        "    val_ratio = 0.15 / 0.85\n",
        "    split_2 = train_val.train_test_split(test_size=val_ratio, seed=seed)\n",
        "    train = split_2[\"train\"]\n",
        "    val = split_2[\"test\"]\n",
        "\n",
        "    return DatasetDict({\"train\": train, \"validation\": val, \"test\": test})\n",
        "\n",
        "# Load Data\n",
        "splits = get_gsm8k_split()\n",
        "train_dataset = splits[\"train\"]\n",
        "val_dataset = splits[\"validation\"]\n",
        "test_dataset = splits[\"test\"]\n",
        "\n",
        "# 4) Evaluate on a small test sample dataset\n",
        "small_val_dataset = val_dataset.select(range(20))\n",
        "\n",
        "# Print dataset information\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")\n",
        "print(f\"Columns: {train_dataset.column_names}\")\n",
        "\n",
        "# Inspect the first training sample\n",
        "print(\"\\nTraining sample:\")\n",
        "print(train_dataset[0])"
      ],
      "metadata": {
        "id": "l1jX_9pv_9MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prolog_helpers.pl"
      ],
      "metadata": {
        "id": "6z7Qiio__9MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prolog_helpers.pl\n",
        ":- module(prolog_helpers, [analyze_code/3]).\n",
        ":- use_module(library(readutil)).\n",
        "\n",
        "%% analyze_code(+File, -PredicateCount, -ConstraintCount)\n",
        "%  Reads the Prolog source in File, counting:\n",
        "%    - Predicates (i.e., top-level clauses) except `solve/1`\n",
        "%    - Curly-brace constraints (anywhere in the term)\n",
        "%  Then prints these counts as:\n",
        "%    PREDICATE_COUNT: <num>\n",
        "%    CONSTRAINT_COUNT: <num>\n",
        "analyze_code(File, PredicateCount, ConstraintCount) :-\n",
        "    open(File, read, Stream),\n",
        "    read_terms(Stream, Terms),\n",
        "    close(Stream),\n",
        "    count_predicates(Terms, PredicateCount),\n",
        "    count_constraints(Terms, ConstraintCount),\n",
        "    format('PREDICATE_COUNT: ~w~n', [PredicateCount]),\n",
        "    format('CONSTRAINT_COUNT: ~w~n', [ConstraintCount]).\n",
        "\n",
        "%% read_terms(+Stream, -Terms)\n",
        "%  Reads terms until end_of_file, returning them in a list.\n",
        "read_terms(Stream, Terms) :-\n",
        "    read_term(Stream, Term, [variable_names(_)]),\n",
        "    ( Term == end_of_file ->\n",
        "         Terms = []\n",
        "    ; read_terms(Stream, Rest),\n",
        "      Terms = [Term|Rest]\n",
        "    ).\n",
        "\n",
        "%% count_predicates(+Terms, -Count)\n",
        "%  Among all top-level clauses, exclude `solve/1`.\n",
        "count_predicates(Terms, Count) :-\n",
        "    include(valid_predicate, Terms, ValidPreds),\n",
        "    length(ValidPreds, Count).\n",
        "\n",
        "valid_predicate(Term) :-\n",
        "    % Skip directives (:- operator) first\n",
        "    \\+ Term = (:- _),\n",
        "    get_head(Term, Head),\n",
        "    nonvar(Head),\n",
        "    Head =.. [Functor|_],\n",
        "    Functor \\= solve.  % Exclude solve/1\n",
        "\n",
        "%% get_head(+Term, -Head)\n",
        "%  If it's (Head :- Body), unify Head. Otherwise, it's a fact, so unify Term.\n",
        "%  Skip directives\n",
        "get_head((Head :- _), Head) :- !.\n",
        "get_head(Head, Head) :-\n",
        "    % Additional check to skip directive heads\n",
        "    \\+ Head = (:- _).\n",
        "\n",
        "%% count_constraints(+Terms, -Count)\n",
        "%  Count all curly-brace constraints in all terms.\n",
        "count_constraints(Terms, Count) :-\n",
        "    aggregate_all(count, (member(Term, Terms), has_constraint(Term)), Count).\n",
        "\n",
        "has_constraint(Term) :-\n",
        "    contains_constraint(Term).\n",
        "\n",
        "%% contains_constraint(+Term)\n",
        "%  Recursively checks sub-terms for { ... } patterns.\n",
        "contains_constraint(Term) :-\n",
        "    nonvar(Term),\n",
        "    (  Term = {_}                % direct curly-brace\n",
        "    ;  Term =.. [_|Args],\n",
        "       member(Arg, Args),\n",
        "       contains_constraint(Arg)\n",
        "    )."
      ],
      "metadata": {
        "id": "LyxbLwCi_9MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multipletry (evaluation)"
      ],
      "metadata": {
        "id": "ws4hn7za_9MG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import re\n",
        "import time\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "from vllm import SamplingParams\n",
        "import os, uuid\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Safer arithmetic evaluation pattern: accepts digits, spaces, and basic arithmetic symbols.\n",
        "ARITHMETIC_PATTERN = re.compile(r'^[\\d\\s+\\-*/().]+$')\n",
        "\n",
        "##############################################################################\n",
        "# Utility: Flatten a list of messages into a single string (system + user)\n",
        "##############################################################################\n",
        "def conversation_to_prompt(messages):\n",
        "    \"\"\"\n",
        "    Keep only the first system message (if any) and the first user message (if any).\n",
        "    \"\"\"\n",
        "    system_msg = None\n",
        "    user_msg = None\n",
        "    for msg in messages:\n",
        "        role = msg.get(\"role\")\n",
        "        if role == \"system\" and system_msg is None:\n",
        "            system_msg = msg\n",
        "        elif role == \"user\" and user_msg is None:\n",
        "            user_msg = msg\n",
        "    prompt_text = \"\"\n",
        "    if system_msg:\n",
        "        prompt_text += f\"(SYSTEM) {system_msg['content']}\\n\"\n",
        "    if user_msg:\n",
        "        prompt_text += f\"(USER) {user_msg['content']}\\n\"\n",
        "    return prompt_text.strip()\n",
        "\n",
        "##############################################################################\n",
        "# 1) Extract the last complete <answer>...</answer> block from text.\n",
        "##############################################################################\n",
        "def extract_xml_answer(text: str) -> str:\n",
        "    try:\n",
        "        # Truncate at <|endoftext|> if it exists.\n",
        "        eot_index = text.find(\"<|endoftext|>\")\n",
        "        truncated_text = text[:eot_index] if eot_index != -1 else text\n",
        "        start = truncated_text.find(\"<answer>\")\n",
        "        if start == -1:\n",
        "            return None\n",
        "        end = truncated_text.find(\"</answer>\", start)\n",
        "        if end == -1:\n",
        "            return None\n",
        "        return truncated_text[start+len(\"<answer>\"):end].strip()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "##############################################################################\n",
        "# 2) Execute Prolog code and return the final line of SWI-Prolog stdout.\n",
        "##############################################################################\n",
        "def execute_prolog_code_subprocess(prolog_code: str, timeout=5) -> str:\n",
        "    temp_file = f\"temp_{uuid.uuid4().hex}.pl\"\n",
        "    try:\n",
        "        with open(temp_file, \"w\") as f:\n",
        "            f.write(prolog_code)\n",
        "        result = subprocess.run(\n",
        "            [\"swipl\", \"-q\", \"-f\", temp_file, \"-g\", \"solve(X), writeln(X), halt\"],\n",
        "            capture_output=True, text=True, timeout=timeout\n",
        "        )\n",
        "        if result.returncode != 0 or not result.stdout:\n",
        "            return None\n",
        "        lines = result.stdout.strip().splitlines()\n",
        "        return lines[-1].strip() if lines else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error executing Prolog code: {e}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if os.path.exists(temp_file):\n",
        "            os.remove(temp_file)\n",
        "\n",
        "##############################################################################\n",
        "# 3) Analyze structure of generated Prolog code using prolog_helpers.pl.\n",
        "##############################################################################\n",
        "def analyze_prolog_structure_subprocess(prolog_code: str) -> dict:\n",
        "    temp_file = f\"temp_{uuid.uuid4().hex}.pl\"\n",
        "    try:\n",
        "        with open(temp_file, \"w\") as f:\n",
        "            f.write(prolog_code)\n",
        "        result = subprocess.run(\n",
        "            [\n",
        "                \"swipl\", \"-q\", \"-f\", \"prolog_helpers.pl\",\n",
        "                \"-g\", f\"analyze_code('{temp_file}', PredCount, ConstCount), halt\"\n",
        "            ],\n",
        "            capture_output=True, text=True, timeout=10\n",
        "        )\n",
        "        predicate_count = 0\n",
        "        constraint_count = 0\n",
        "        for line in result.stdout.splitlines():\n",
        "            line = line.strip()\n",
        "            if line.startswith(\"PREDICATE_COUNT:\"):\n",
        "                predicate_count = int(line.split(\":\", 1)[1].strip())\n",
        "            elif line.startswith(\"CONSTRAINT_COUNT:\"):\n",
        "                constraint_count = int(line.split(\":\", 1)[1].strip())\n",
        "        return {\"predicate_count\": predicate_count, \"constraint_count\": constraint_count}\n",
        "    except Exception as e:\n",
        "        print(\"Error in analyze_prolog_structure_subprocess:\", e)\n",
        "        return {\"predicate_count\": 0, \"constraint_count\": 0}\n",
        "    finally:\n",
        "        if os.path.exists(temp_file):\n",
        "            os.remove(temp_file)\n",
        "\n",
        "##############################################################################\n",
        "# 4) Check structural correctness: valid if at least one predicate (other than solve/1)\n",
        "#    and at least one curly-brace constraint exist.\n",
        "##############################################################################\n",
        "def check_structure_correctness(prolog_code: str) -> bool:\n",
        "    if not prolog_code:\n",
        "        return False\n",
        "    analysis = analyze_prolog_structure_subprocess(prolog_code)\n",
        "    pred_count = analysis.get(\"predicate_count\", 0)\n",
        "    const_count = analysis.get(\"constraint_count\", 0)\n",
        "    return (pred_count >= 1) and (const_count >= 1)\n",
        "\n",
        "##############################################################################\n",
        "# 5. Reward Functions\n",
        "##############################################################################\n",
        "# 5.1 Semantic Similarity Reward (Direct Approach):\n",
        "def semantic_similarity_reward(completions, answer, semantic_model, **kwargs) -> list[float]:\n",
        "    \"\"\"\n",
        "    Computes a semantic similarity score between generated and reference Prolog code.\n",
        "    Returns a score on a [0,1] scale.\n",
        "    \"\"\"\n",
        "    extracted_responses = [extract_xml_answer(comp[0][\"content\"]) for comp in completions]\n",
        "    rewards = []\n",
        "    for model_code, ref_code in zip(extracted_responses, answer):\n",
        "        if not model_code or not ref_code:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        try:\n",
        "            embedding_model = semantic_model.encode(model_code, convert_to_tensor=True)\n",
        "            embedding_ref = semantic_model.encode(ref_code, convert_to_tensor=True)\n",
        "            cosine_sim = util.cos_sim(embedding_model, embedding_ref).item()\n",
        "            preds_model = set(re.findall(r'(\\w+)\\(', model_code))\n",
        "            preds_ref = set(re.findall(r'(\\w+)\\(', ref_code))\n",
        "            pred_overlap = len(preds_model & preds_ref) / max(1, len(preds_ref))\n",
        "            reward_val = (cosine_sim + pred_overlap) / 2.0\n",
        "            rewards.append(reward_val)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in semantic similarity: {str(e)}\")\n",
        "            rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "# 5.2 Correctness Reward (Numeric Evaluation)\n",
        "def correctness_reward_func(prompts, completions, answer, numerical_result, **kwargs) -> list[float]:\n",
        "    responses = [comp[0][\"content\"] for comp in completions]\n",
        "    extracted_responses = [extract_xml_answer(r) for r in responses]\n",
        "    correct_values = numerical_result\n",
        "    if len(responses) > 0:\n",
        "        question = prompts[0][-1][\"content\"] if (prompts and prompts[0]) else \"N/A\"\n",
        "        print(\"-\" * 20,\n",
        "              f\"Question:\\n{question}\",\n",
        "              f\"\\nReference Prolog answer:\\n{answer[0]}\",\n",
        "              f\"\\nReference Numerical Result:\\n{correct_values[0]}\",\n",
        "              f\"\\nModel Response:\\n{responses[0]}\",\n",
        "              f\"\\nExtracted Code:\\n{extracted_responses[0]}\")\n",
        "    model_values = []\n",
        "    for code in extracted_responses:\n",
        "        if code:\n",
        "            mv = execute_prolog_code_subprocess(code)\n",
        "            if mv is None:\n",
        "                print(\"SWI-Prolog returned no output or an error.\")\n",
        "            model_values.append(mv)\n",
        "        else:\n",
        "            model_values.append(None)\n",
        "            print(\"No Prolog code extracted from the model.\")\n",
        "    rewards = []\n",
        "    for mv, cv in zip(model_values, correct_values):\n",
        "        if mv is None or cv is None:\n",
        "            rewards.append(0.5)\n",
        "            print(\"Partial Reward: Code missing or no numeric match.\")\n",
        "            continue\n",
        "        try:\n",
        "            if mv.startswith(\"_\"):\n",
        "                rewards.append(0.5)\n",
        "                print(f\"Unbound variable in Prolog output: {mv}\")\n",
        "                continue\n",
        "            mv_cleaned = mv.strip().split('\\n')[-1]\n",
        "            mv_float = float(mv_cleaned)\n",
        "            cv_float = float(cv)\n",
        "            print(f\"Model Value: {mv_float}, Correct Value: {cv_float}\")\n",
        "            if abs(mv_float - cv_float) < 1e-6:\n",
        "                rewards.append(2.0)\n",
        "                print(\"Match: Model value is correct.\")\n",
        "            else:\n",
        "                rewards.append(1.0)\n",
        "                print(\"Partial Reward: Numeric result incorrect.\")\n",
        "        except Exception as e:\n",
        "            rewards.append(0.5)\n",
        "            print(f\"Error converting output to float: {e}\\nModel: {mv}, Correct: {cv}\")\n",
        "    return rewards\n",
        "\n",
        "# 5.3 Prolog Structure Reward\n",
        "def prolog_structure_reward_func(completions, **kwargs) -> list[float]:\n",
        "    rewards = []\n",
        "    for comp in completions:\n",
        "        text = comp[0][\"content\"]\n",
        "        start = text.find(\"<answer>\")\n",
        "        if start == -1:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        end = text.find(\"</answer>\", start)\n",
        "        if end == -1:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "        extracted_code = text[start+len(\"<answer>\"):end].strip()\n",
        "        analysis = analyze_prolog_structure_subprocess(extracted_code)\n",
        "        pred_count = analysis.get(\"predicate_count\", 0)\n",
        "        const_count = analysis.get(\"constraint_count\", 0)\n",
        "        score = min(pred_count * 0.25, 0.75) + min(const_count * 0.3, 0.9)\n",
        "        hardcode_regex = r'solve\\([^)]*\\)\\s*:-.*(\\b\\w+\\s*=\\s*\\d+|{\\s*\\w+\\s*=\\s*\\d+\\s*})'\n",
        "        if re.search(hardcode_regex, extracted_code, flags=re.DOTALL):\n",
        "            score *= 0.2\n",
        "        final_score = max(0.0, min(score, 2.0))\n",
        "        rewards.append(final_score)\n",
        "    return rewards\n",
        "\n",
        "# 5.4 Prolog Syntax and XML Reward Functions (unchanged)\n",
        "def prolog_syntax_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r'(?::-|solve\\s*\\(|use_module|clpq|\\.\\s*$)'\n",
        "    rewards = []\n",
        "    for comp in completions:\n",
        "        text = comp[0][\"content\"]\n",
        "        hits = re.findall(pattern, text, re.MULTILINE)\n",
        "        score = min(len(hits) * 0.2, 1.0)\n",
        "        rewards.append(score)\n",
        "    return rewards\n",
        "\n",
        "def strict_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n",
        "    responses = [comp[0][\"content\"] for comp in completions]\n",
        "    matches = [re.match(pattern, r, flags=re.DOTALL) for r in responses]\n",
        "    return [0.5 if m else 0.0 for m in matches]\n",
        "\n",
        "def soft_format_reward_func(completions, **kwargs) -> list[float]:\n",
        "    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n",
        "    responses = [comp[0][\"content\"] for comp in completions]\n",
        "    matches = [re.search(pattern, r, flags=re.DOTALL) for r in responses]\n",
        "    return [0.5 if m else 0.0 for m in matches]\n",
        "\n",
        "def count_xml(text: str) -> float:\n",
        "    count = 0.0\n",
        "    if text.count(\"<reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n</reasoning>\\n\") == 1:\n",
        "        count += 0.125\n",
        "    if text.count(\"\\n<answer>\\n\") == 1:\n",
        "        count += 0.125\n",
        "        count -= len(text.split(\"\\n</answer>\\n\")[-1]) * 0.001\n",
        "    if text.count(\"\\n</answer>\") == 1:\n",
        "        count += 0.125\n",
        "        count -= (len(text.split(\"\\n</answer>\\n\")[-1]) - 1) * 0.001\n",
        "    return count\n",
        "\n",
        "def xmlcount_reward_func(completions, **kwargs) -> list[float]:\n",
        "    contents = [comp[0][\"content\"] for comp in completions]\n",
        "    return [count_xml(c) for c in contents]\n",
        "\n",
        "##############################################################################\n",
        "# 6) Main Evaluation Function with Multiple Try Logic\n",
        "##############################################################################\n",
        "def extract_reasoning(text: str) -> str:\n",
        "    try:\n",
        "        start = text.find(\"<reasoning>\")\n",
        "        if start == -1:\n",
        "            return None\n",
        "        end = text.find(\"</reasoning>\", start)\n",
        "        if end == -1:\n",
        "            return None\n",
        "        return text[start+len(\"<reasoning>\"):end].strip()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def evaluate_prolog_generation(model, tokenizer, dataset, max_new_tokens=1024, max_attempts=20):\n",
        "    # Initialize semantic similarity model\n",
        "    semantic_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Initialize metrics\n",
        "    metrics = {\n",
        "        'total_samples': 0,\n",
        "        'strict_correct': 0,\n",
        "        'arithmetic_correct': 0,\n",
        "        'structure_correct': 0,\n",
        "        'full_correct': 0,\n",
        "        'overall_count': 0,\n",
        "        'strict_count': 0,\n",
        "        'arithmetic_count': 0,\n",
        "        'structure_count': 0,\n",
        "        'full_correct_count': 0,\n",
        "        'semantic_scores': [],\n",
        "        'semantic_sum': 0.0,\n",
        "        'total_semantic': 0.0,\n",
        "        'attempts_list': [],\n",
        "        'generation_times': [],\n",
        "        'prolog_times': [],\n",
        "        'validation_times': [],\n",
        "    }\n",
        "\n",
        "    # Initialize WandB table for PER-ATTEMPT details\n",
        "    results_table = wandb.Table(columns=[\n",
        "        \"Sample Index\", \"Question\", \"Reference Answer\", \"Gold Numerical Result\",\n",
        "        \"Attempt Number\", \"Is Final Successful Attempt\",\n",
        "        \"Model Output\", \"Extracted Code\", \"Execution Result\",\n",
        "        \"Is Valid Prolog\", \"Produces Number\", \"Is Correct Number (vs Gold)\",\n",
        "        \"Is Structure Valid\", \"Generation Time (s)\", \"Prolog Execution Time (s)\",\n",
        "        \"Failure Reason\"\n",
        "    ])\n",
        "\n",
        "    # Add sampling parameters definition\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=0.2,\n",
        "        top_p=0.95,\n",
        "        max_tokens=max_new_tokens,\n",
        "        stop=[\"</answer>\"],\n",
        "        include_stop_str_in_output=True\n",
        "    )\n",
        "\n",
        "    # Add start time tracking\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, sample in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
        "        sample_index = idx + 1\n",
        "        prompt_text = \"\"\n",
        "        model_text = \"\"\n",
        "        prolog_code = \"\"\n",
        "        final_line = None\n",
        "        gold_str = \"\"\n",
        "        error_type = None\n",
        "        is_strict = False\n",
        "        is_arithmetic = False\n",
        "        is_structure = False\n",
        "        is_semantic = False\n",
        "        is_full_correct = False\n",
        "        semantic_score = 0.0\n",
        "        raw_semantic = 0.0\n",
        "        generation_time = 0.0\n",
        "        prolog_exec_time = 0.0\n",
        "        validation_time = 0.0\n",
        "        attempts = 0\n",
        "        success = False\n",
        "        all_attempts = []\n",
        "        successful_attempt_number = -1 # Track which attempt succeeded\n",
        "\n",
        "        try:\n",
        "            messages = sample[\"prompt\"]\n",
        "            prompt_text = conversation_to_prompt(messages)\n",
        "            print(\"\\n[1] Flattened Prompt:\\n\", prompt_text) # <-- Added prompt print\n",
        "            gold_str = sample.get(\"numerical_result\") # Get gold value once per sample\n",
        "            gold_val = None\n",
        "            if gold_str:\n",
        "                try:\n",
        "                    gold_val = float(gold_str)\n",
        "                except ValueError:\n",
        "                    print(f\"Warning: Could not convert gold numerical result '{gold_str}' to float for sample {sample_index}\")\n",
        "\n",
        "            # --- Attempt Loop ---\n",
        "            while attempts < max_attempts and not success:\n",
        "                attempts += 1\n",
        "                gen_start = time.time()\n",
        "                output_data = model.fast_generate(prompt_text, sampling_params)\n",
        "                generation_time_attempt = time.time() - gen_start\n",
        "                metrics['generation_times'].append(generation_time_attempt)\n",
        "                gen_model_text = output_data[0].outputs[0].text\n",
        "                gen_prolog_code = extract_xml_answer(gen_model_text)\n",
        "\n",
        "                # Initialize attempt_info\n",
        "                attempt_info = {\n",
        "                    'attempt_number': attempts,\n",
        "                    'model_output': gen_model_text,\n",
        "                    'extracted_code': gen_prolog_code if gen_prolog_code else \"No code extracted\",\n",
        "                    'execution_result': None,\n",
        "                    'is_valid_prolog': False,\n",
        "                    'produces_number': False,\n",
        "                    'is_correct_number': False, # Correctness vs gold standard\n",
        "                    'structure_valid': False,\n",
        "                    'generation_time': generation_time_attempt,\n",
        "                    'prolog_execution_time': None,\n",
        "                    'reason_for_failure': None\n",
        "                }\n",
        "\n",
        "                if not gen_prolog_code:\n",
        "                    attempt_info['reason_for_failure'] = \"No Prolog code extracted\"\n",
        "                    all_attempts.append(attempt_info)\n",
        "                    print(f\"Attempt {attempts}: No Prolog code extracted.\")\n",
        "                    continue\n",
        "\n",
        "                # Execute code\n",
        "                prolog_start = time.time()\n",
        "                gen_final_line = execute_prolog_code_subprocess(gen_prolog_code)\n",
        "                prolog_exec_time_attempt = time.time() - prolog_start\n",
        "                metrics['prolog_times'].append(prolog_exec_time_attempt) # Still collect total time\n",
        "\n",
        "                # Update attempt info\n",
        "                attempt_info['execution_result'] = gen_final_line\n",
        "                attempt_info['is_valid_prolog'] = gen_final_line is not None\n",
        "                attempt_info['prolog_execution_time'] = prolog_exec_time_attempt\n",
        "                attempt_info['structure_valid'] = check_structure_correctness(gen_prolog_code)\n",
        "\n",
        "                # Check if execution yielded a number and if it's correct\n",
        "                try:\n",
        "                    if gen_final_line:\n",
        "                        float_result = float(gen_final_line)\n",
        "                        attempt_info['produces_number'] = True\n",
        "                        if gold_val is not None:\n",
        "                             attempt_info['is_correct_number'] = abs(float_result - gold_val) < 1e-6\n",
        "\n",
        "                        # SUCCESS CONDITION: Execution produced a number\n",
        "                        model_text = gen_model_text # Store the successful output\n",
        "                        prolog_code = gen_prolog_code # Store the successful code\n",
        "                        final_line = gen_final_line # Store the successful result\n",
        "                        generation_time = generation_time_attempt # Store successful gen time\n",
        "                        prolog_exec_time = prolog_exec_time_attempt # Store successful exec time\n",
        "                        print(f\"Attempt {attempts}: Successful numeric output: {final_line}\")\n",
        "                        success = True\n",
        "                        successful_attempt_number = attempts # Record which attempt succeeded\n",
        "                    else:\n",
        "                        attempt_info['reason_for_failure'] = \"Prolog execution did not return a result\"\n",
        "                except ValueError:\n",
        "                    attempt_info['reason_for_failure'] = \"Prolog output is not a valid number\"\n",
        "                    print(f\"Attempt {attempts}: Prolog code did not yield a numeric result ('{gen_final_line}').\")\n",
        "                except Exception as e:\n",
        "                    attempt_info['reason_for_failure'] = f\"Error checking result: {str(e)}\"\n",
        "                    print(f\"Attempt {attempts}: Error checking result: {str(e)}\")\n",
        "\n",
        "                all_attempts.append(attempt_info)\n",
        "\n",
        "                if success:\n",
        "                    break\n",
        "            # --- End Attempt Loop ---\n",
        "\n",
        "            # (5) Validate numeric correctness (for overall sample metrics)\n",
        "            valid_start = time.time()\n",
        "            is_strict = False\n",
        "            is_arithmetic = False\n",
        "            if success and gold_val is not None: # Check strict/arithmetic only if an attempt succeeded\n",
        "                try:\n",
        "                    prolog_val = float(final_line)\n",
        "                    is_strict = abs(prolog_val - gold_val) < 1e-6\n",
        "                except:\n",
        "                     pass # is_strict remains False\n",
        "\n",
        "                if not is_strict and ARITHMETIC_PATTERN.match(final_line.strip()):\n",
        "                    try:\n",
        "                        # Use a safer eval if needed, or stick to float conversion if sufficient\n",
        "                        eval_val = float(final_line.strip()) # Simpler if only numbers expected\n",
        "                        is_arithmetic = abs(eval_val - gold_val) < 1e-6\n",
        "                    except Exception as e:\n",
        "                        error_type = f\"Arithmetic conversion error: {str(e)}\"\n",
        "            validation_time = time.time() - valid_start # Time for this specific check\n",
        "\n",
        "            # (6) Structural correctness (for overall sample metrics)\n",
        "            is_structure = False\n",
        "            if success and prolog_code: # Check structure only if an attempt succeeded\n",
        "                is_structure = check_structure_correctness(prolog_code)\n",
        "\n",
        "            # (7) Semantic similarity calculation (based on successful attempt)\n",
        "            reference_answer = [sample.get(\"answer\", \"\")]\n",
        "            has_reference = bool(reference_answer[0].strip()) if reference_answer else False\n",
        "            raw_semantic = 0.0\n",
        "            if success and has_reference:\n",
        "                completion_wrapper = [[{\"content\": model_text}]]\n",
        "                try:\n",
        "                    semantic_rewards = semantic_similarity_reward(\n",
        "                        completion_wrapper,\n",
        "                        reference_answer,\n",
        "                        semantic_model=semantic_model\n",
        "                    )\n",
        "                    raw_semantic = semantic_rewards[0] if semantic_rewards else 0.0\n",
        "                except Exception as e:\n",
        "                    print(f\"Semantic similarity error: {str(e)}\")\n",
        "                    raw_semantic = 0.0\n",
        "\n",
        "            # (8) Overall correctness metric\n",
        "            is_full_correct = (is_strict or is_arithmetic) and is_structure\n",
        "\n",
        "            # Print summary for the *sample*\n",
        "            print(f\"\\n--- Sample {sample_index} Summary ---\")\n",
        "            print(f\"Result achieved in attempt: {successful_attempt_number if success else 'N/A'} / {attempts}\")\n",
        "            if success: # <-- Add this block to print successful output\n",
        "                print(\"-\" * 40)\n",
        "                print(\"Successful Model Output:\")\n",
        "                print(model_text.strip())\n",
        "                print(\"-\" * 40)\n",
        "            print(f\"Final Prolog Output: {final_line or 'None'}\")\n",
        "            print(f\"Gold Value: {gold_str or 'None'}\")\n",
        "            print(f\"Strict: {is_strict} | Arithmetic: {is_arithmetic} | Structure: {is_structure} | Full: {is_full_correct}\")\n",
        "            print(f\"Semantic Score: {raw_semantic*100:.2f}%\")\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            error_type = f\"Processing error: {str(e)}\"\n",
        "            print(f\"DEBUG: Exception during sample {sample_index} processing:\", error_type)\n",
        "            # Ensure all_attempts has at least a placeholder if error occurred before loop\n",
        "            if not all_attempts:\n",
        "                 all_attempts.append({'attempt_number': 1, 'reason_for_failure': error_type,\n",
        "                                      'model_output': 'ERROR', 'extracted_code': 'ERROR',\n",
        "                                      # ... add other keys with default/error values ...\n",
        "                                     })\n",
        "\n",
        "        # --- Log EACH attempt to WandB Table ---\n",
        "        for attempt_data in all_attempts:\n",
        "            results_table.add_data(\n",
        "                sample_index,\n",
        "                prompt_text,\n",
        "                sample.get(\"answer\", \"\"), # Reference Prolog code\n",
        "                gold_str or \"\",           # Gold numerical result as string\n",
        "                attempt_data['attempt_number'],\n",
        "                # Mark True only if this attempt is the one that succeeded\n",
        "                attempt_data['attempt_number'] == successful_attempt_number,\n",
        "                attempt_data['model_output'],\n",
        "                attempt_data['extracted_code'],\n",
        "                str(attempt_data['execution_result']) if attempt_data['execution_result'] is not None else \"\",\n",
        "                attempt_data['is_valid_prolog'],\n",
        "                attempt_data['produces_number'],\n",
        "                attempt_data['is_correct_number'], # Correctness vs Gold for this attempt\n",
        "                attempt_data['structure_valid'],\n",
        "                f\"{attempt_data['generation_time']:.3f}\",\n",
        "                f\"{attempt_data['prolog_execution_time']:.3f}\" if attempt_data['prolog_execution_time'] is not None else \"\",\n",
        "                str(attempt_data['reason_for_failure']) if attempt_data['reason_for_failure'] is not None else \"\"\n",
        "            )\n",
        "\n",
        "        # --- Update and Log Aggregate Metrics (per sample) ---\n",
        "        metrics['total_samples'] += 1\n",
        "        metrics['attempts_list'].append(attempts) # Log total attempts for this sample\n",
        "        if is_strict: metrics['strict_correct'] += 1\n",
        "        if is_arithmetic: metrics['arithmetic_correct'] += 1\n",
        "        if is_structure: metrics['structure_correct'] += 1\n",
        "        if is_full_correct: metrics['full_correct_count'] += 1\n",
        "        if has_reference:\n",
        "            metrics['semantic_scores'].append(raw_semantic)\n",
        "            metrics['semantic_sum'] += raw_semantic\n",
        "            if raw_semantic >= 0.7:  # Threshold for \"good\" semantic similarity\n",
        "                metrics['total_semantic'] += 1\n",
        "\n",
        "        # Calculate running accuracies\n",
        "        accuracies = {\n",
        "             'strict': (metrics['strict_correct'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0,\n",
        "             'arithmetic': (metrics['arithmetic_correct'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0,\n",
        "             'structure': (metrics['structure_correct'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0,\n",
        "             'full_correct': (metrics['full_correct_count'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0,\n",
        "        }\n",
        "\n",
        "        # Print running accuracies for the current sample\n",
        "        print(f\"Accuracies => Prolog: {accuracies['strict']:.2f}%, \"\n",
        "              f\"Arithmetic: {accuracies['arithmetic']:.2f}%, \"\n",
        "              f\"Structure: {accuracies['structure']:.2f}%, \"\n",
        "              f\"Fully Correct: {accuracies['full_correct']:.2f}%\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Log live aggregate metrics to WandB charts\n",
        "        wandb.log({\n",
        "            \"live/prolog_acc\": accuracies['strict'],\n",
        "            \"live/arithmetic_acc\": accuracies['arithmetic'],\n",
        "            \"live/structure_acc\": accuracies['structure'],\n",
        "            \"live/full_correct_acc\": accuracies['full_correct'],\n",
        "            \"live/semantic_score\": raw_semantic * 100, # Semantic score for the sample (if successful)\n",
        "            \"live/avg_attempts\": sum(metrics['attempts_list']) / len(metrics['attempts_list']) if metrics['attempts_list'] else 0,\n",
        "            \"time/generation_successful\": generation_time if success else 0,\n",
        "            \"time/prolog_exec_successful\": prolog_exec_time if success else 0,\n",
        "            \"time/validation\": validation_time,\n",
        "            \"errors\": 1 if error_type else 0,\n",
        "            \"sample_total_attempts\": attempts\n",
        "        }, step=sample_index)\n",
        "\n",
        "    # --- Final Calculations and Logging ---\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    # Safety check\n",
        "    if metrics['total_samples'] == 0:\n",
        "        print(\"WARNING: No samples processed during evaluation\")\n",
        "        return {\"accuracies\": {}, \"timing\": {}, \"details\": []}\n",
        "\n",
        "    # Calculate final average times (using all collected times)\n",
        "    avg_times = {\n",
        "        'generation': sum(metrics['generation_times'])/len(metrics['generation_times']) if metrics['generation_times'] else 0.0,\n",
        "        'prolog': sum(metrics['prolog_times'])/len(metrics['prolog_times']) if metrics['prolog_times'] else 0.0,\n",
        "        'validation': sum(metrics['validation_times'])/len(metrics['validation_times']) if metrics['validation_times'] else 0.0\n",
        "    }\n",
        "\n",
        "    # Calculate final aggregate accuracies\n",
        "    final_accuracies = {\n",
        "        'strict': (metrics['strict_correct'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0,\n",
        "        'arithmetic': (metrics['arithmetic_correct'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0,\n",
        "        'structure': (metrics['structure_correct'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0,\n",
        "        'full_correct': (metrics['full_correct_count'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0,\n",
        "    }\n",
        "    avg_semantic = metrics['semantic_sum'] / metrics['total_samples'] if metrics['total_samples'] > 0 else 0.0\n",
        "    final_semantic_accuracy = (metrics['total_semantic'] / metrics['total_samples'] * 100) if metrics['total_samples'] > 0 else 0.0\n",
        "\n",
        "    # Log the detailed PER-ATTEMPT table ONCE at the end\n",
        "    wandb.log({\n",
        "        \"detailed_results_per_attempt\": results_table,\n",
        "        \"final/prolog_accuracy\": final_accuracies['strict'],\n",
        "        \"final/arithmetic_accuracy\": final_accuracies['arithmetic'],\n",
        "        \"final/structure_accuracy\": final_accuracies['structure'],\n",
        "        \"final/full_correct_accuracy\": final_accuracies['full_correct'],\n",
        "        \"final/semantic_accuracy\": final_semantic_accuracy,\n",
        "        \"final/avg_semantic_score\": avg_semantic,\n",
        "        \"final/total_time\": elapsed,\n",
        "        \"final/avg_generation_time_per_attempt\": avg_times['generation'],\n",
        "        \"final/avg_prolog_time_per_attempt\": avg_times['prolog'],\n",
        "    })\n",
        "\n",
        "    # Update WandB Summary with final aggregates\n",
        "    wandb.summary.update({\n",
        "        \"prolog_accuracy\": final_accuracies['strict'],\n",
        "        \"arithmetic_accuracy\": final_accuracies['arithmetic'],\n",
        "        \"structure_accuracy\": final_accuracies['structure'],\n",
        "        \"full_correct_accuracy\": final_accuracies['full_correct'],\n",
        "        \"semantic_accuracy\": final_semantic_accuracy,\n",
        "        \"avg_semantic_score\": avg_semantic,\n",
        "        \"avg_generation_time_per_attempt\": avg_times['generation'],\n",
        "        \"avg_prolog_time_per_attempt\": avg_times['prolog'],\n",
        "    })\n",
        "\n",
        "    # Print final summary to console\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" EVALUATION COMPLETE \".center(80))\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Prolog Accuracy: {final_accuracies['strict']:.2f}%\")\n",
        "    print(f\"Arithmetic Accuracy: {final_accuracies['arithmetic']:.2f}%\")\n",
        "    print(f\"Structure Accuracy: {final_accuracies['structure']:.2f}%\")\n",
        "    print(f\"Fully Correct Accuracy: {final_accuracies['full_correct']:.2f}%\")\n",
        "    print(f\"Semantic Accuracy (>= threshold): {final_accuracies['structure']:.2f}%\")\n",
        "    print(f\"Average Semantic Score: {avg_semantic:.2f}\")\n",
        "    print(f\"\\nAverage Times (per attempt):\")\n",
        "    print(f\"  Generation: {avg_times['generation']:.3f}s\")\n",
        "    print(f\"  Prolog Execution: {avg_times['prolog']:.3f}s\")\n",
        "    print(f\"\\nTotal Evaluation Time: {elapsed:.2f} seconds\")\n",
        "    return {\n",
        "        \"accuracies\": final_accuracies,\n",
        "        \"timing\": avg_times,\n",
        "        \"details\": results_table.data\n",
        "    }\n",
        "\n",
        "##############################################################################\n",
        "# 7) Example usage\n",
        "##############################################################################\n",
        "if __name__ == \"__main__\":\n",
        "    wandb.init(\n",
        "        project=\"gsm8k-prolog-prover-new-evaluation\",\n",
        "        name=\"sp-struct-rwd1-multipletry\",\n",
        "        settings=wandb.Settings(start_method=\"thread\"),\n",
        "        config={\"environment\": \"colab\"}\n",
        "    )\n",
        "\n",
        "    result_stats = evaluate_prolog_generation(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        val_dataset\n",
        "    )\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "QfNqEYtv_9MG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
